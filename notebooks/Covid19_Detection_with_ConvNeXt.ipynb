{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRQ6TsKuSW1W",
        "outputId": "10a2a81d-dda4-4260-ad89-4de4beb301bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.8/825.8 KB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.9/178.9 KB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb pytorch-lightning timm -qqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9pJ49LSTh-V"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "# your favorite machine learning tracking tool\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from torchmetrics import Accuracy\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "os.environ['KAGGLE_USERNAME'] = \"adhisetiawan\"\n",
        "os.environ['KAGGLE_KEY'] = \"96ce54a3b358a4b04a716080f32292ec\"\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "import zipfile\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cj2a1F0yTsqx",
        "outputId": "763e3b6a-e5a3-4d6c-e3d7-789bf79c406d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login f1503a61cc5475492484bf5123e2afc5c715ac55\n",
        "# wandb.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x51ZBuTzztsI"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUeZIgHZz1xu"
      },
      "outputs": [],
      "source": [
        "class Covid19DataModule(pl.LightningDataModule):\n",
        "    def __init__(self, batch_size, data_dir: str = './'):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Augmentation policy for training set\n",
        "        self.augmentation = transforms.Compose([\n",
        "              transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "              transforms.RandomRotation(degrees=15),\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.CenterCrop(size=224),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        # Preprocessing steps applied to validation and test set.\n",
        "        self.transform = transforms.Compose([\n",
        "              transforms.Resize(size=256),\n",
        "              transforms.CenterCrop(size=224),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        self.num_classes = 3\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.api = KaggleApi()\n",
        "        self.api.authenticate()\n",
        "        self.api.dataset_download_files('pranavraikokte/covid19-image-dataset', path=\".\")\n",
        "\n",
        "        with zipfile.ZipFile('/content/covid19-image-dataset.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # build dataset\n",
        "        self.train = datasets.ImageFolder('/content/Covid19-dataset/train')\n",
        "        self.val = datasets.ImageFolder('/content/Covid19-dataset/test')\n",
        "        self.test = datasets.ImageFolder('/content/Covid19-dataset/test/')\n",
        "\n",
        "        self.train.transform = self.augmentation\n",
        "        self.val.transform = self.transform\n",
        "        self.test.transform = self.transform\n",
        "        \n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val, batch_size=self.batch_size, num_workers=2)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        return DataLoader(self.test, batch_size=self.batch_size, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euArOd2fQJcX"
      },
      "source": [
        "## Image Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9HR_-yCcEpu"
      },
      "outputs": [],
      "source": [
        "class ImagePredictionLogger(pl.Callback):\n",
        "    def __init__(self, val_samples, num_samples=3):\n",
        "        super().__init__()\n",
        "        self.val_imgs, self.val_labels = val_samples\n",
        "        self.val_imgs = self.val_imgs[:num_samples]\n",
        "        self.val_labels = self.val_labels[:num_samples]\n",
        "          \n",
        "    def on_validation_epoch_end(self, trainer, pl_module):\n",
        "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
        "\n",
        "        logits = pl_module(val_imgs)\n",
        "        preds = torch.argmax(logits, 1)\n",
        "\n",
        "        trainer.logger.experiment.log({\n",
        "            \"examples\": [wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
        "                            for x, pred, y in zip(val_imgs, preds, self.val_labels)],\n",
        "            \"global_step\": trainer.global_step\n",
        "            })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwY6LvI2VfrT"
      },
      "source": [
        "# LightingModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ti9KbCR5Q6DU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "class Block(nn.Module):\n",
        "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "    \n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
        "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    r\"\"\" ConvNeXt\n",
        "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
        "          https://arxiv.org/pdf/2201.03545.pdf\n",
        "    Args:\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        num_classes (int): Number of classes for classification head. Default: 1000\n",
        "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
        "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans=3, num_classes=1000, \n",
        "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
        "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
        "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError \n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EV8_rsq4Q_ir"
      },
      "outputs": [],
      "source": [
        "model_urls = {\n",
        "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
        "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
        "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
        "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
        "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
        "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
        "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
        "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
        "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
        "}\n",
        "\n",
        "@register_model\n",
        "def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_small(pretrained=False,in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_xlarge(pretrained=False, in_22k=True, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
        "    if pretrained:\n",
        "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
        "        url = model_urls['convnext_xlarge_22k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0xf25aUckF"
      },
      "outputs": [],
      "source": [
        "class ConvNeXtLitModel(pl.LightningModule):\n",
        "    def __init__(self, input_shape, num_classes, learning_rate=1e-3, transfer=False, model_name=\"convnext_base\"):\n",
        "        super().__init__()\n",
        "        \n",
        "        # log hyperparameters\n",
        "        self.save_hyperparameters()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.dim = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.model_name = model_name\n",
        "        \n",
        "        # transfer learning if pretrained=True\n",
        "        self.feature_extractor = convnext_small(pretrained=transfer)\n",
        "\n",
        "        if transfer:\n",
        "            # layers are frozen by using eval()\n",
        "            self.feature_extractor.eval()\n",
        "            # freeze params\n",
        "            for param in self.feature_extractor.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "        n_sizes = self._get_conv_output(input_shape)\n",
        "\n",
        "        self.classifier = nn.Linear(n_sizes, num_classes)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
        "  \n",
        "    # returns the size of the output tensor going into the Linear layer from the conv block.\n",
        "    def _get_conv_output(self, shape):\n",
        "        batch_size = 1\n",
        "        tmp_input = torch.autograd.Variable(torch.rand(batch_size, *shape))\n",
        "\n",
        "        output_feat = self._forward_features(tmp_input) \n",
        "        n_size = output_feat.data.view(batch_size, -1).size(1)\n",
        "        return n_size\n",
        "        \n",
        "    # returns the feature tensor from the conv block\n",
        "    def _forward_features(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x\n",
        "    \n",
        "    # will be used during inference\n",
        "    def forward(self, x):\n",
        "       x = self._forward_features(x)\n",
        "       x = x.view(x.size(0), -1)\n",
        "       x = self.classifier(x)\n",
        "       \n",
        "       return x\n",
        "    \n",
        "    def training_step(self, batch):\n",
        "        batch, gt = batch[0], batch[1]\n",
        "        out = self.forward(batch)\n",
        "        loss = self.criterion(out, gt)\n",
        "\n",
        "        acc = self.accuracy(out, gt)\n",
        "\n",
        "        self.log(\"train/loss\", loss)\n",
        "        self.log(\"train/acc\", acc)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        batch, gt = batch[0], batch[1]\n",
        "        out = self.forward(batch)\n",
        "        loss = self.criterion(out, gt)\n",
        "\n",
        "        self.log(\"val/loss\", loss)\n",
        "\n",
        "        acc = self.accuracy(out, gt)\n",
        "        self.log(\"val/acc\", acc)\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        batch, gt = batch[0], batch[1]\n",
        "        out = self.forward(batch)\n",
        "        loss = self.criterion(out, gt)\n",
        "        \n",
        "        return {\"loss\": loss, \"outputs\": out, \"gt\": gt}\n",
        "    \n",
        "    def test_epoch_end(self, outputs):\n",
        "        loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
        "        output = torch.cat([x['outputs'] for x in outputs], dim=0)\n",
        "        \n",
        "        gts = torch.cat([x['gt'] for x in outputs], dim=0)\n",
        "\n",
        "        torchmodel = \"model_final.pth\"\n",
        "        torch.save(model.state_dict(), torchmodel)\n",
        "        artifact = wandb.Artifact(name=f\"model_{self.model_name}.ckpt\", type=\"model\")\n",
        "        artifact.add_file(torchmodel)\n",
        "        wandb.log_artifact(artifact)\n",
        "        \n",
        "        self.log(\"test/loss\", loss)\n",
        "        acc = self.accuracy(output, gts)\n",
        "        self.log(\"test/acc\", acc)\n",
        "        \n",
        "        self.test_gts = gts\n",
        "        self.test_output = output\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCGQbJq7Wx6o"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "93ef8ea324eb4ae7b1822c4bd4fa1318",
            "d7628d037e2e4fa1bc68e64079ad8955",
            "539eb09e114a4d4b91b4cf2cea59006d",
            "7836b634946645bda102e1cd92769c26",
            "3c2406ff95fe41e7b6eaa70fe77c096e",
            "5bfe99ad85d14984953e04f99ffd5804",
            "98b2ffde218c42268956fd4a81349024",
            "bc60c00962fa4624b31ce7228035d705",
            "9d5f10f01c81484f94a9cca848de69b3",
            "3b7b2b6d75eb4cf6ba58cf97c703e50a",
            "fd47b5a0f94f4e9b8b53450b51c2bacd"
          ]
        },
        "id": "Oq4R84NbWcH6",
        "outputId": "095c13c4-9a1b-4fbb-a3dd-67585d751971"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93ef8ea324eb4ae7b1822c4bd4fa1318",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/338M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        }
      ],
      "source": [
        "dm = Covid19DataModule(batch_size=32)\n",
        "dm.prepare_data()\n",
        "dm.setup()\n",
        "\n",
        "samples = next(iter(dm.val_dataloader()))\n",
        "\n",
        "\n",
        "MODEL_NAME = \"convnext_base\"\n",
        "model = ConvNeXtLitModel((3, 224, 224), 3, transfer=True, model_name=MODEL_NAME)\n",
        "trainer = pl.Trainer(\n",
        "    logger=WandbLogger(project=\"test-kaggel-download\", name=MODEL_NAME),\n",
        "    log_every_n_steps=8,\n",
        "    max_epochs=100, \n",
        "    accelerator=\"gpu\",\n",
        "    deterministic=True, \n",
        "    callbacks=[ImagePredictionLogger(samples)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "InyEyFeqXcck"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model, dm)\n",
        "trainer.test(model, dm)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3b7b2b6d75eb4cf6ba58cf97c703e50a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c2406ff95fe41e7b6eaa70fe77c096e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "539eb09e114a4d4b91b4cf2cea59006d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc60c00962fa4624b31ce7228035d705",
            "max": 354476359,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d5f10f01c81484f94a9cca848de69b3",
            "value": 354476359
          }
        },
        "5bfe99ad85d14984953e04f99ffd5804": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7836b634946645bda102e1cd92769c26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b7b2b6d75eb4cf6ba58cf97c703e50a",
            "placeholder": "​",
            "style": "IPY_MODEL_fd47b5a0f94f4e9b8b53450b51c2bacd",
            "value": " 338M/338M [01:24&lt;00:00, 2.20MB/s]"
          }
        },
        "93ef8ea324eb4ae7b1822c4bd4fa1318": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d7628d037e2e4fa1bc68e64079ad8955",
              "IPY_MODEL_539eb09e114a4d4b91b4cf2cea59006d",
              "IPY_MODEL_7836b634946645bda102e1cd92769c26"
            ],
            "layout": "IPY_MODEL_3c2406ff95fe41e7b6eaa70fe77c096e"
          }
        },
        "98b2ffde218c42268956fd4a81349024": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d5f10f01c81484f94a9cca848de69b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc60c00962fa4624b31ce7228035d705": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7628d037e2e4fa1bc68e64079ad8955": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bfe99ad85d14984953e04f99ffd5804",
            "placeholder": "​",
            "style": "IPY_MODEL_98b2ffde218c42268956fd4a81349024",
            "value": "100%"
          }
        },
        "fd47b5a0f94f4e9b8b53450b51c2bacd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}